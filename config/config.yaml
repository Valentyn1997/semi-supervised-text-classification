# Model params
model: # Model params
  model_type: bert # Model type selected in the list
  model_name_or_path: bert-base-uncased # Path to pre-trained model or shortcut name selected in the list  # ["bert-large-uncased", "bert-large-cased", "bert-base-uncased", "bert-base-cased"],
  config_name: bert-base-uncased # Pre-trained config name or path if not the same as model_name
  tokenizer_name: bert-base-uncased # Pre-trained tokenizer name or path if not the same as model_name
  cache_dir: # Where do you want to store the pre-trained models downloaded from s3
  do_lower_case: True  # Set this flag if you are using an uncased model

# Data params
data:
  test_id:  # The topic id of the test_set. Set value -1 for random selection from all topics. ["abortion", "cloning", "death_penalty", "gun_control", "marijuana_legalization", "minimum_wage", "nuclear_energy", "school_uniforms"]
  max_seq_length: 60  # The maximum total input sequence length after tokenization. Sequences longer than this will be truncated,
                        # sequences shorter will be padded.
  mask: # ???
  validation_size: 0.15  # The size of the validation set based on the train-set
  batch_size:  # Batch size per GPU/CPU for training
    train: 10
    val: 20
    test: 20

# Optimizer
optimizer:
  weight_decay: 0.0  # Weight decay if we apply some
  adam_epsilon: 1e-8  # Epsilon for Adam optimizer
  learning_rate: 2e-5  # The initial learning rate for Adam
  warmup_steps: 0   # Linear warmup over warmup_steps
  max_grad_norm: 1.0   # Max gradient norm

# Experiment
task_name: SD  # The name of the task to train selected in the list
max_steps: -1   # If > 0: set total number of training steps to perform. Override num_train_epochs
num_train_epochs: 2  # Total number of training epochs to perform.
early_stopping_patience: 5  #
gradient_accumulation_steps: 1  # Number of updates steps to accumulate before performing a backward/update pass
seed: 45  # random seed for initialization
overwrite_cache: True  # Overwrite the cached training and evaluation sets
checkpoint: False  # Checkpoint weights, optimizer state to MlFlow

# Hydra defaults
defaults:
  - hydra/job_logging: colorlog
  - hydra/hydra_logging: colorlog